{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.080540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.946373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.000144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.643663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.780747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.955021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob  pred\n",
       "0    0.048101     0\n",
       "1    0.092385     0\n",
       "2    0.080540     0\n",
       "3    0.020636     0\n",
       "4    0.001234     0\n",
       "..        ...   ...\n",
       "295  0.946373     1\n",
       "296  0.000144     0\n",
       "297  0.643663     1\n",
       "298  0.780747     1\n",
       "299  0.955021     1\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('C:\\Git\\Artifact-Detection-In-ECG\\label\\RTFACT_241001_Public_Possibility_T#08.xlsx')\n",
    "\n",
    "predict = pd.DataFrame()\n",
    "predict['prob'] = df['prob']\n",
    "predict['pred'] = df['pred']\n",
    "\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI Score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 average = `binary`(default) / AUROC average = `macro`(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nbr = 1000000\n",
    "num_ones = 75\n",
    "num_zeros = 300 - num_ones\n",
    "\n",
    "unique_arrays = set()\n",
    "\n",
    "while len(unique_arrays) < nbr:\n",
    "    array = np.array([1] * num_ones + [0] * num_zeros)\n",
    "    np.random.shuffle(array)\n",
    "    unique_arrays.add(tuple(array))\n",
    "\n",
    "unique_arrays_list = [np.array(arr) for arr in unique_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [33:12<00:00, 501.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(\"F1 score average = binary(default)\")\n",
    "# print(\"AUROC score average = macro(default)\\n\")\n",
    "maybe_true = pd.DataFrame()\n",
    "for idx in tqdm(range(len(unique_arrays_list))):\n",
    "    maybe_true['label'] = unique_arrays_list[idx]\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(maybe_true['label'], predict['pred'])\n",
    "\n",
    "    # AUROC 계산\n",
    "    auroc = roc_auc_score(maybe_true['label'], predict['prob'])\n",
    "\n",
    "    # MCC 계산\n",
    "    mcc = matthews_corrcoef(maybe_true['label'], predict['pred'])\n",
    "\n",
    "    # CPI 계산\n",
    "    cpi = 0.25 * f1 + 0.25 * auroc + 0.5 * mcc\n",
    "\n",
    "    # print(f\"====== CPI estimate ===============\")\n",
    "    # print(f\"- F1 Score: {f1}\")\n",
    "    # print(f\"- AUROC: {auroc}\")\n",
    "    # print(f\"- MCC: {mcc}\")\n",
    "    # print(f\">>> CPI: {cpi}\")\n",
    "\n",
    "    CPI_Score = np.round(cpi,3)\n",
    "    if np.abs(cpi - 0.468) < 0.003:\n",
    "        print(f'idx : {idx}, CPI Score : {CPI_Score}')\n",
    "        maybe_true.to_excel(f\"C:\\Git\\Artifact-Detection-In-ECG\\label\\maybe_true_public_test_label_{idx}.xlsx\")\n",
    "        # print(unique_arrays_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11152954it [7:35:47, 549.01it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, matthews_corrcoef\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "filePath = 'C:\\Git\\Artifact-Detection-In-ECG\\label\\RTFACT_241001_Public_Possibility_T#08.xlsx'\n",
    "df = pd.read_excel(filePath)\n",
    "\n",
    "predict = pd.DataFrame()\n",
    "predict['prob'] = df['prob']\n",
    "predict['pred'] = df['pred']\n",
    "\n",
    "predict\n",
    "\n",
    "# Total number of points\n",
    "n_total = 300\n",
    "\n",
    "# Function to compute CPI\n",
    "def compute_cpi(y_true, y_pred, y_prob):\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auroc = roc_auc_score(y_true, y_prob)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # CPI formula\n",
    "    cpi = 0.25 * f1 + 0.25 * auroc + 0.5 * mcc\n",
    "    return cpi\n",
    "\n",
    "# Generate the ground truth labels (225 zeros and 75 ones)\n",
    "# Brute force approach: find a y_true that results in a CPI of 0.49\n",
    "from itertools import permutations\n",
    "\n",
    "y_true_base = [0]*225 + [1]*75\n",
    "\n",
    "best_y_true = None\n",
    "best_cpi = 0\n",
    "\n",
    "# Try to find a permutation of y_true that gives a CPI of 0.49\n",
    "for y_true_candidate in tqdm(permutations(y_true_base)):\n",
    "    cpi = compute_cpi(y_true_candidate, predict['pred'], predict['prob'])\n",
    "    if abs(cpi - 0.49) < 0.001:  # Check if CPI is close to 0.49\n",
    "        best_y_true = y_true_candidate\n",
    "        best_cpi = cpi\n",
    "        print(best_y_true, best_cpi)\n",
    "        maybe_true.to_excel(f\"C:\\Git\\Artifact-Detection-In-ECG\\label\\maybe_true_public_test_label_{best_cpi}.xlsx\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
